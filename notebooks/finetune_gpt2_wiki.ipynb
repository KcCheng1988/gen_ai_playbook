{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune a Language Model\n",
    "___\n",
    "\n",
    "#### Types of Language Model\n",
    "\n",
    "##### Causal Language Model\n",
    "1. The model has to predict the next token in the sentence. \n",
    "2. To make sure that the model does not cheat, it gets an attention mask that will prevent it to access the tokens after the $(i+1)$-th position.\n",
    "\n",
    "##### Masked Language Model\n",
    "1. The model has to predict some tokens that are masked in the input.\n",
    "2. It still has access to the whole sentence, so it can use the tokens before and after the masked tokens to predict their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import ClassLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Dataset\n",
    "___\n",
    "1. We will use the `Wikitext 2` datasets as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/kccheng1988/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9875fbc7c51c4634b5302c47e592ec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' The game \\'s battle system , the BliTZ system , is carried over directly from Valkyira Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters \\' turns . Each character has a field and distance of movement limited by their Action Gauge . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant boons to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special Abilities that grant them temporary boosts on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without depleting his Action Point gauge , the character Reila can shift into her \" Valkyria Form \" and become invincible , while Imca can target multiple enemy units with her heavy weapon . \\n'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a sample of the dataset\n",
    "datasets['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function shows some randomly picked sample\n",
    "def show_random_elements(dataset, num_examples = 10):\n",
    "    assert num_examples <= len(dataset)\n",
    "\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i : typ.names[i])\n",
    "    display(HTML(df.to_html())) # Renders a dataframe as an HTML table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= = Common names = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Peshkin 's findings show a \" total world \" where the lessons of religion and education are intertwined into an \" interrelated , interdependent \" philosophy . The academy 's intent is to make Christian professionals as what Peshkin describes as \" a vocational school directed to work in the Lord 's service \" . When compared to the work of public schools , the private school 's instructors said both kinds of institutions impose a lifestyle and set of values as a kind of \" brainwashing \" . Peshkin notes that while students \" largely identify with \" and uphold the fundamentalist teachings , they permit themselves the option of having \" individual interpretations \" and minor beliefs . Some students either dissent against the academy 's rules or are regarded as too pious , but most students are moderate . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2O + XeO \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= = Fate of the DuMont stations = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The evacuation by train from Romani was carried out in a manner which caused much suffering and shock to the wounded . It was not effected till the night of August 6 – the transport of prisoners of war being given precedence over that of the wounded – and only open trucks without straw were available . The military exigencies necessitated shunting and much delay , so that five hours were occupied on the journey of twenty @-@ five miles . It seemed a cruel shame to shunt a train full of wounded in open trucks , but it had to be done . Every bump in our springless train was extremely painful . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fieldfare is omnivorous . Animal food in the diet includes snails and slugs , earthworms , spiders and insects such as beetles and their larvae , flies and grasshoppers . When berries ripen in the autumn these are taken in great number . Hawthorn , holly , rowan , yew , juniper , dog rose , Cotoneaster , Pyracantha and Berberis are all relished . Later in the winter windfall apples are eaten , swedes attacked in the field and grain and seeds eaten . When these are exhausted , or in particularly harsh weather , the birds may move to marshes or even the foreshore where molluscs are to be found . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>= = = Modern era = = = \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "1. Some of the texts are full paragraphs of a Wikipedia article, while others are just titles or empty lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Language Modeling\n",
    "___\n",
    "* We are going to take all the texts in our dataset and concatenate them after they are tokenized.\n",
    "* Then, we split them in examples of a certain sequence length.\n",
    "* This way, the model will receive chunks of contiguous text that may look like:\n",
    "\n",
    "`part of text 1` <br>\n",
    "`end of text 1 [BOS_TOKEN] beginning of text 2`\n",
    "\n",
    "* We will use `distilgpt2` model for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'distilgpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kccheng1988/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-7a451edd1b9d6a54_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/kccheng1988/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-47d068062a7caf0a_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/kccheng1988/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8f73ed8d3cd21d84_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "# We can now call the tokenizer on our texts\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched = True, num_proc = 4, remove_columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View a sample of the tokenized datasets\n",
    "tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes:\n",
    "* Now, we need to concatenate all our texts together then split the result in small chunks of a certain `block_size`.\n",
    "* We will use `map` again, with the option `batched = True`\n",
    "* This option lets us change the number of examples in the datasets by returning a different number of examples than we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum input length of model:  1024\n"
     ]
    }
   ],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "print('Maximum input length of model: ', block_size)\n",
    "\n",
    "# If the maximum input length is too big to fit in your GPU RAM, we will take a bit less\n",
    "block_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Group texts -- ##\n",
    "def group_texts(examples):\n",
    "    # Using sum(ls, []) to concatenate a list of lists.\n",
    "    concatenated_examples = {k : sum(ls, []) for k, ls in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # We drop the small residual block in the end.\n",
    "    # We could add padding if the model supports it (customizable)\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k : [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
